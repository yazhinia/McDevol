{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159956, 32)\n",
      "0.014136075973510742 self index adding\n",
      "19052.436 138.03056 distance cutoff\n",
      "4.717833042144775 seconds for graph and densities calculation\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import heapq\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from connected_components import group_indices\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "KUPPA = 0.4 # rough idea of how many density peaks given C total contigs\n",
    "TOTAL_CONTIGS = 0\n",
    "LATENT_DIMENSION = 0\n",
    "ETA = 0 # 1/latent_dim weight of contigs length\n",
    "DISTANCE_CUTOFF = 1\n",
    "NN_SIZE_CUTOFF = 200000\n",
    "NCPUS = (os.cpu_count()-2 if os.cpu_count() is not None else 8)\n",
    "\n",
    "def add_selfindex(data_indices, labels, distances):\n",
    "    \"\"\" add self index where missed by hnswlib \"\"\"\n",
    "    s2 = time.time()\n",
    "    # in case query index does not found as first nearest neighbor,\n",
    "    # insert manually and remove last nearest neighbor\n",
    "    add_selfinds = np.nonzero(labels.astype(int)[:,0] != data_indices)[0]\n",
    "    labels[add_selfinds] = np.insert(labels[add_selfinds], 0, add_selfinds, axis=1)[:,:-1]\n",
    "    distances[add_selfinds] = np.insert(distances[add_selfinds], 0, 0.0, axis=1)[:,:-1]\n",
    "    print(time.time()-s2, 'self index adding')\n",
    "    return labels, distances\n",
    "\n",
    "def get_neighbors(latent, length):\n",
    "    \"\"\" Get neighbors and distance cutoff\"\"\"\n",
    "    s= time.time()\n",
    "    data_indices = np.arange(TOTAL_CONTIGS)\n",
    "    k_nn = int(TOTAL_CONTIGS ** KUPPA)\n",
    "\n",
    "    p = hnswlib.Index(space = 'l2', dim = LATENT_DIMENSION)\n",
    "    # M - max. number of out-edges ef_construction\n",
    "    # ef_construction - the size of the dynamic list for the nearest neighbors\n",
    "    # and controls index_time/accuracy. Important during indexing time\n",
    "    p.init_index(max_elements = TOTAL_CONTIGS, ef_construction = 200, M = 16)\n",
    "    p.set_ef(k_nn) # number should be greater than k_nn\n",
    "    p.set_num_threads(NCPUS)\n",
    "    p.add_items(latent)\n",
    "\n",
    "    retry = True\n",
    "    while retry:\n",
    "        labels, distances = p.knn_query(latent, k=k_nn)\n",
    "        cumsum_length = np.cumsum(length[labels], axis=1)\n",
    "        contigs_with_enough_neighbors = np.nonzero(cumsum_length[:,-1] > NN_SIZE_CUTOFF)[0]\n",
    "        retry = len(contigs_with_enough_neighbors) / TOTAL_CONTIGS < 0.5\n",
    "        if retry:\n",
    "            k_nn *= 2 # adjustable\n",
    "            p.set_ef(k_nn) # adjustable\n",
    "\n",
    "    labels, distances = add_selfindex(data_indices, labels, distances)\n",
    "    dist_cutoff_indices = np.argmax(\n",
    "            np.cumsum(length[labels], axis=1) > NN_SIZE_CUTOFF, axis=1)\n",
    "    indices_tofind_dist = np.nonzero(dist_cutoff_indices)[0]\n",
    "    # for long contigs dist_cutoff_indices would be zero. Hence, add manually\n",
    "    ind_check_longcontig = np.nonzero(dist_cutoff_indices==0)[0]\n",
    "    indices_to_add = np.nonzero(length[ind_check_longcontig]> NN_SIZE_CUTOFF)[0]\n",
    "    indices_tofind_dist = np.concatenate((indices_tofind_dist, \\\n",
    "                                        ind_check_longcontig[indices_to_add]))\n",
    "    \n",
    "    # Find distance cutoff\n",
    "    global DISTANCE_CUTOFF\n",
    "    DISTANCE_CUTOFF = np.median(\n",
    "        distances[indices_tofind_dist,dist_cutoff_indices[indices_tofind_dist]])\n",
    "    print(DISTANCE_CUTOFF, np.sqrt(DISTANCE_CUTOFF), 'distance cutoff')\n",
    "\n",
    "    valid_indices = [[] for _ in range(TOTAL_CONTIGS)]\n",
    "    \n",
    "    # flags to indicate contigs found all nearest neighbors within DISTANCE_CUTOFF\n",
    "    flags = distances[:,-1] > DISTANCE_CUTOFF\n",
    "    c_indices = np.where(flags==1)[0]\n",
    "\n",
    "    def update_nnlists(c_indices, nn_lists, nn_dists):\n",
    "        # Create a mask for distances less than DISTANCE_CUTOFF\n",
    "        mask = nn_dists <= DISTANCE_CUTOFF\n",
    "        # Apply the mask to nn_inds_k\n",
    "        filtered_inds = np.where(mask, nn_lists, -1).astype(int)  # Using -1 as a placeholder for invalid indices\n",
    "        for num, i in enumerate(c_indices):\n",
    "            valid_indices[i] = filtered_inds[num, filtered_inds[num] >= 0]\n",
    "\n",
    "    update_nnlists(c_indices, labels[flags], distances[flags])\n",
    "\n",
    "    k_nn_more = k_nn\n",
    "    remaining_indices = np.where(flags==0)[0]\n",
    "    \n",
    "    while len(remaining_indices) > 0:\n",
    "        k_nn_more *= 2\n",
    "        p.set_ef(k_nn_more)\n",
    "        nn_lists, nn_dists = p.knn_query(latent[remaining_indices], k = k_nn_more)\n",
    "        flags_check = nn_dists[:,-1] > DISTANCE_CUTOFF\n",
    "        indices_toupdate = np.where(flags_check==1)[0]\n",
    "        # if indices_toupdate.size > 0:\n",
    "        if k_nn_more < 10000 and indices_toupdate.size > 0:\n",
    "            c_indices = remaining_indices[indices_toupdate]\n",
    "            remaining_indices = remaining_indices[~flags_check]\n",
    "            update_nnlists(c_indices, nn_lists[indices_toupdate], nn_dists[indices_toupdate])\n",
    "        else:\n",
    "            # stop if k_nn_more increases to impede knn_query and \\\n",
    "            # compute distance with all points for remaining contigs to \\\n",
    "            # valid indices\n",
    "            distances_remaining = np.sum((latent - latent[remaining_indices][:,np.newaxis])**2,axis=2)\n",
    "            for row_distance, inds in zip(distances_remaining, remaining_indices):\n",
    "                nn_inds = np.argsort(row_distance)\n",
    "                nn_inds = nn_inds[np.where(row_distance[nn_inds] <= DISTANCE_CUTOFF)]\n",
    "                valid_indices[inds] = nn_inds\n",
    "            break\n",
    "\n",
    "    del p\n",
    "\n",
    "    return valid_indices, labels, distances\n",
    "\n",
    "def get_density_peaks(valid_indices, labels, distances, length):\n",
    "    \"\"\" compute density using k-nearest neighbors \"\"\"\n",
    "    s = time.time()\n",
    "\n",
    "    # # Create a mask for distances less than DISTANCE_CUTOFF\n",
    "    # mask = distances <= DISTANCE_CUTOFF\n",
    "\n",
    "    # # Apply the mask to nn_inds_k\n",
    "    # filtered_inds = np.where(mask, labels, -1).astype(int)  # Using -1 as a placeholder for invalid indices\n",
    "    # del mask\n",
    "    # valid_indices = [filtered_inds[c, filtered_inds[c] >= 0] for c in range(TOTAL_CONTIGS)]\n",
    "    # del filtered_inds\n",
    "\n",
    "    densities = np.array([np.sum(length[valid_indices[c]] ** ETA) / DISTANCE_CUTOFF for c in range(TOTAL_CONTIGS)])\n",
    "    \n",
    "    graph = {}\n",
    "    nearest = np.arange(TOTAL_CONTIGS)\n",
    "    density_peaks_flag = np.full(TOTAL_CONTIGS,-1)\n",
    "    \n",
    "    for c, valid_inds in enumerate(valid_indices):\n",
    "        # single member clusters\n",
    "        if valid_inds.size == 1:\n",
    "            # print(densities[valid_inds], 'densities of single member', c)\n",
    "            if densities[valid_inds] >= NN_SIZE_CUTOFF: # long genomic contig as a single cluster\n",
    "                density_peaks_flag[c] = c\n",
    "                graph[c] = []\n",
    "                nearest[c] = c\n",
    "            else: # short isolated contig\n",
    "                # maxdens_neighbor = np.argmax(densities[labels[c]])\n",
    "                # if distances[c][maxdens_neighbor] <= DISTANCE_CUTOFF * 1.5 :\n",
    "                #     graph[c] = labels[c][maxdens_neighbor]\n",
    "                #     nearest[c] = labels[c][maxdens_neighbor]\n",
    "                # else:\n",
    "                #     graph[c] = []\n",
    "                #     nearest[c] = labels[c][1]\n",
    "\n",
    "                # HyperParam 1.8\n",
    "                nearest_points = np.nonzero(distances[c] <= DISTANCE_CUTOFF * 1.8)[0]\n",
    "                if nearest_points.size > 1:\n",
    "                    near_pointdensities = densities[labels[c][nearest_points]]\n",
    "                    # Maybe applicable only for Toy datasets with counts as densities\n",
    "                    # if np.all(near_pointdensities == near_pointdensities[0]):\n",
    "                    #     graph[c] = labels[c][1]\n",
    "                    #     nearest[c] = labels[c][1]\n",
    "                    # else:\n",
    "                    maxdens_neighbor = nearest_points[np.argmax(near_pointdensities)]\n",
    "                    graph[c] = labels[c][maxdens_neighbor]\n",
    "                    nearest[c] = labels[c][maxdens_neighbor]\n",
    "                else:\n",
    "                    graph[c] = []\n",
    "                    nearest[c] = labels[c][1]\n",
    "\n",
    "            continue\n",
    "        \n",
    "        max_densityindex = valid_inds[np.argmax(densities[valid_inds])]\n",
    "        # valid_inds all can have the same density but small. They become separate low-density peaks\n",
    "        # TODO: have to solve to merge such low-density peaks with higher density peaks\n",
    "        if c == max_densityindex:\n",
    "            density_peaks_flag[c] = c\n",
    "            nearest[c] = c\n",
    "        else:\n",
    "            nearest[c] = max_densityindex\n",
    "            \n",
    "        graph[c] = list(zip(valid_inds[1:], densities[valid_inds[1:]]))\n",
    "\n",
    "    print(time.time() - s, 'seconds for graph and densities calculation')\n",
    "    density_peaks_inds = np.nonzero(density_peaks_flag>=0)[0]\n",
    "    \n",
    "    peak_count = int(TOTAL_CONTIGS ** (1-KUPPA))\n",
    "\n",
    "    if peak_count > len(density_peaks_inds): \n",
    "        peak_count = len(density_peaks_inds)\n",
    "    \n",
    "    get_inds = np.argsort(densities[density_peaks_inds])[::-1][:peak_count]\n",
    "    density_peaks = density_peaks_inds[get_inds]\n",
    "\n",
    "    # plt.figure(figsize=(20,12))\n",
    "    # plt.plot(densities)\n",
    "\n",
    "    return densities, density_peaks, graph, nearest\n",
    "\n",
    "def dijkstra_max_min_density(graph, start):\n",
    "    \"\"\" return max density path between start and nearest neighbors \"\"\"    \n",
    "    \n",
    "    # Initialize maximum minimum densities and priority queue\n",
    "    max_min_densities = {node: float('-inf') for node in graph}\n",
    "    max_min_densities[start] = float('inf')\n",
    "    priority_queue = [(-max_min_densities[start], start)]  # Priority queue of (negative density, node)\n",
    "\n",
    "    while priority_queue:\n",
    "        # Extract node with highest maximum minimum density\n",
    "        current_density, current_node = heapq.heappop(priority_queue)\n",
    "        current_density = -current_density\n",
    "\n",
    "        for neighbor, density in graph[current_node]:\n",
    "            # Calculate new maximum minimum density for neighbor\n",
    "            new_density = min(current_density, density)\n",
    "            if new_density > max_min_densities[neighbor]:\n",
    "                max_min_densities[neighbor] = new_density\n",
    "\n",
    "                heapq.heappush(priority_queue, (-new_density, neighbor))\n",
    "\n",
    "    return max_min_densities\n",
    "\n",
    "def find_connected_components(merge_links):\n",
    "\n",
    "    K = len(merge_links)\n",
    "    merge_sets = np.zeros(K).astype(int) - 1\n",
    "    merge_curr = 0\n",
    "    for k in np.arange(K):\n",
    "\n",
    "        if merge_sets[k] < 0:\n",
    "\n",
    "            candidates = merge_links[k]\n",
    "\n",
    "            merge_sets[k] = merge_curr\n",
    "\n",
    "            while len(candidates) != 0:\n",
    "                l = candidates.pop(-1)\n",
    "\n",
    "                if merge_sets[l] < 0:\n",
    "                    merge_sets[l] = merge_curr\n",
    "                    candidates.extend(merge_links[l])\n",
    "\n",
    "            merge_curr += 1\n",
    "\n",
    "    return merge_sets\n",
    "\n",
    "def remove_outliers_peaks(data, m=2.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d / (mdev if mdev else 1.)\n",
    "    return np.where(s < m)[0] # indices of outlier points\n",
    "\n",
    "def density_links(args):\n",
    "    i, peak, graph, density_peaks, densities = args\n",
    "    print(peak, i, 'peak and i')\n",
    "    max_min_densities = dijkstra_max_min_density(graph, peak)\n",
    "    higherdensity_links = {k:v for k, v in max_min_densities.items() if (k in density_peaks) and (k != peak) and (v != float('-inf'))}\n",
    "    if higherdensity_links:\n",
    "        max_key = max(higherdensity_links, key=higherdensity_links.get)\n",
    "        separability_index = 1 - (higherdensity_links[max_key] / densities[peak])\n",
    "        merge_peakinds = np.where(np.isin(density_peaks, list(higherdensity_links.keys())))[0]\n",
    "        merge_link = [i] + list(merge_peakinds)\n",
    "    else:\n",
    "        separability_index = 1\n",
    "        merge_link = []\n",
    "    return i, separability_index, merge_link\n",
    "\n",
    "\n",
    "def cluster(latent, contig_length):\n",
    "    \"\"\" cluster contigs \"\"\"\n",
    "\n",
    "    valid_indices, labels, distances = get_neighbors(latent, contig_length)\n",
    "    # print(valid_indices, 'labels and distances')\n",
    "    densities, density_peaks, graph, nearest = get_density_peaks(valid_indices, labels, distances, contig_length)\n",
    "    # print(len(density_peaks), 'total density peaks')\n",
    "    del labels, distances\n",
    "    separability_indices = np.full(len(density_peaks), 1.0, dtype=np.float32)\n",
    "\n",
    "    # peak_neighbors = [valid_indices[i].tolist() for i in density_peaks]\n",
    "    # print(graph, 'graph')\n",
    "    \n",
    "    # connected_peakcomponents = group_indices(peak_neighbors)\n",
    "    \n",
    "    # # from connected_components import select_subgraph\n",
    "    # # subgraph = select_subgraph(graph, connected_peakcomponents[0][1])\n",
    "    # # print(subgraph, 'subgraph')\n",
    "    # subgraph1 = {node: [adj_node for adj_node in graph[node] if adj_node in connected_peakcomponents[0][1]] for node in connected_peakcomponents[0][1]}\n",
    "    # print(subgraph1, 'subgraph1')\n",
    "    # merge_links = []\n",
    "    # for i, peak in enumerate(density_peaks):\n",
    "    #     max_min_densities = dijkstra_max_min_density(graph, peak)\n",
    "\n",
    "    #     higherdensity_links = {k:v for k, v in max_min_densities.items() if (k in density_peaks) and (k != peak) and (v != float('-inf'))}\n",
    "    #     # higherdensity_links = {k:v for k, v in higherdensity_links.items() if v > densities[peak]} # this doesn't work when v and densities[peak] is same value\n",
    "    #     if higherdensity_links:\n",
    "    #         # print(peak, higherdensity_links)\n",
    "    #         max_key = max(higherdensity_links, key=higherdensity_links.get)\n",
    "    #         separability_indices[i] = 1 - (higherdensity_links[max_key] / densities[peak])\n",
    "    #         merge_peakinds = np.where(np.isin(density_peaks, list(higherdensity_links.keys())))[0]\n",
    "    #         merge_links.append([i] + list(merge_peakinds))\n",
    "    #     else:\n",
    "    #         merge_links.append([])\n",
    "\n",
    "    # # args_list = [(i, peak, graph, density_peaks, densities) for i, peak in enumerate(density_peaks)]\n",
    "    # # merge_links = []\n",
    "\n",
    "    # # with Pool(processes=NCPUS) as pool:\n",
    "    # #     results = pool.map(density_links, args_list)\n",
    "\n",
    "    # # for i, separability_index, merge_link in results:\n",
    "    # #     separability_indices[i] = separability_index\n",
    "    # #     merge_links.append(merge_link)\n",
    "    \n",
    "    # # # not useful for deciding number of clusters and assignment as we have merging of density peaks\n",
    "    # plt.figure(figsize=(16,12))\n",
    "    # plt.scatter(densities[density_peaks], separability_indices)\n",
    "    # plt.xlabel('density')\n",
    "    # plt.ylabel('separability index')\n",
    "\n",
    "    # separability_indices = np.sort(separability_indices)[::-1]\n",
    "\n",
    "    # cluster_centercounts = np.argmax(np.abs(np.diff(separability_indices))) + 1\n",
    "    # if cluster_centercounts == 1:\n",
    "    #     cluster_centercounts = len(density_peaks)\n",
    "   \n",
    "    # # # cluster_centers = density_peaks[densities[density_peaks] > int(TOTAL_CONTIGS ** KUPPA)] # density_peaks[:cluster_centercounts]\n",
    "    # # ##################################\n",
    "    \n",
    "    # merge_sets = find_connected_components(merge_links)\n",
    "\n",
    "    # return densities, density_peaks, merge_sets, nearest, cluster_centercounts\n",
    "\n",
    "\n",
    "def assign_points(density_peaks, merge_sets, nearest, densities):\n",
    "    \"\"\" assign points to density peaks and merge closer density peaks \"\"\"\n",
    "    nearest[density_peaks] = density_peaks\n",
    "    \n",
    "    nearest_prev = np.zeros(nearest.size, dtype=int) - 1\n",
    "\n",
    "    while (nearest != nearest_prev).any():\n",
    "        nearest_prev = nearest\n",
    "        nearest = nearest[nearest[nearest[nearest[:]]]]\n",
    "\n",
    "    components = []\n",
    " \n",
    "    for k in range(len(density_peaks)):\n",
    "        \n",
    "        if len((np.argwhere(nearest == density_peaks[k])[0])) > 0:\n",
    "            components.append(np.nonzero(nearest == density_peaks[k])[0])\n",
    "\n",
    "        else:\n",
    "            raise RuntimeWarning(\"no clusters in cluster_centers[k] is assigned to nearest[k]\")\n",
    "\n",
    "    clusters = []\n",
    "\n",
    "    unassigned_points = np.setdiff1d(nearest, density_peaks)\n",
    "    # print(unassigned_points, labels[unassigned_points], 'assignment step unassigned points')\n",
    "\n",
    "    for i in set(merge_sets):\n",
    "        merge_indices = np.nonzero(merge_sets == i)[0]\n",
    "        merge_temp = []\n",
    "        for j in merge_indices:\n",
    "            merge_temp.extend(components[j])\n",
    "        clusters.append(merge_temp)\n",
    "    \n",
    "    # for f in clusters:\n",
    "    #     print(len(f), sum(densities[f]))\n",
    "        \n",
    "    return clusters, components\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    variable = '/Users/yazhini/Documents/work/'\n",
    "    latent = np.load(variable + 'latent_mu.npy')\n",
    "    contig_length = np.load(variable + 'contigs_2klength.npz',allow_pickle=True)['arr_0']\n",
    "    # contig_names = np.load(variable + 'contigs_2knames.npz',allow_pickle=True)['arr_0']\n",
    "    otuids = np.loadtxt(variable + 'otuids', dtype='object')\n",
    "    \n",
    "    TOTAL_CONTIGS, LATENT_DIMENSION = latent.shape\n",
    "    ETA = 0.4 # 1 / LATENT_DIMENSION\n",
    "\n",
    "    print(latent.shape)\n",
    "    cluster(latent, contig_length)\n",
    "    # densities, density_peaks, merge_sets, nearest, cluster_centercounts = cluster(latent, contig_length)\n",
    "    # clusters, components = assign_points(density_peaks, merge_sets, nearest, densities)\n",
    "    # labels = np.zeros(TOTAL_CONTIGS,dtype=int)\n",
    "\n",
    "    # counter = 1\n",
    "    # for i in range(len(clusters)):\n",
    "    #     labels[clusters[i]] = counter\n",
    "    #     counter += 1\n",
    "\n",
    "    # for j in range(len(labels)):\n",
    "    #     print(labels[j], otuids[j])\n",
    "\n",
    "    # import seaborn as sns\n",
    "    # import plotly.express as px\n",
    "    # fig = px.scatter(x=latent[:,8], y=latent[:,4])\n",
    "    # fig.update_traces(marker=dict(size=1))\n",
    "    # fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
