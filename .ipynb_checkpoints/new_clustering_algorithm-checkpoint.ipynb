{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering initiated\n",
      "\n",
      "alpha optimized is: 8.445274503124452 and took 0.8554117679595947 seconds\n",
      "\n",
      "Obtained 37 clusters from initial clustering\n",
      "Initial clustering took: 2.337231397628784 seconds\n",
      "count_numbers_of_sharedcontigs took  0.8554587364196777 seconds\n",
      "find_connected_components took  0.0004763603210449219 seconds\n",
      "[2 1 2 1 2 1 2 7 1 1 1 1 1 1 1 1 2 1 1 2 3 2]\n",
      "number of connected components 22\n",
      "count_by_connecting_centroids took  4.058035612106323 seconds\n",
      "overall time taken for new clustering is:  4.066399812698364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazhini/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from optimize_alpha import optimize_alpha\n",
    "from distance_calculations import distance\n",
    "\n",
    "\n",
    "def compute_distance(dat, c, Rc, N, an, alpha):\n",
    "    # pool = Pool()\n",
    "    dist = distance(dat[:,c], dat, Rc[c], Rc, N, an, alpha)\n",
    "    return dist\n",
    "\n",
    "def cluster_by_centroids(dat, d0, N, C, Rc, Rn):\n",
    "\n",
    "    members = []\n",
    "    cluster_curr = 0\n",
    "    cluster_assigned = np.zeros(C) - 1\n",
    "    dists = []\n",
    "\n",
    "    s = time.time()\n",
    "    a     = optimize_alpha(dat, Rc, Rn, N)\n",
    "    an    = a * Rn / Rn.sum()\n",
    "    alpha = an.sum()\n",
    "\n",
    "    print(\"alpha optimized is:\",a, \"and took\", time.time() - s, \"seconds\\n\")\n",
    "\n",
    "    s = time.time()\n",
    "    iterate_ind = np.argsort(Rc)[::-1]\n",
    "    for c in iterate_ind:\n",
    "    # for c in np.arange(C):\n",
    "\n",
    "        if cluster_assigned[c] < 0 :\n",
    "\n",
    "            dist = compute_distance(dat, c, Rc, N, an, alpha)\n",
    "            reassign_ind = np.intersect1d(np.nonzero(cluster_assigned < 0)[0], np.nonzero(np.array(dist < d0))[0])\n",
    "            members.append(reassign_ind)\n",
    "            cluster_assigned[reassign_ind] = cluster_curr\n",
    "            dists.append([dist])\n",
    "            cluster_curr += 1\n",
    "\n",
    "    # with open(\"/big/work/metadevol/benchmark_dataset1/initial_clusters_new\", 'a') as file:\n",
    "    #     for f in range(len(members)):\n",
    "    #         for q in members[f]:\n",
    "    #             file.write(str(q) + \" \" + str(f) + \"\\n\")\n",
    "\n",
    "    print(\"Obtained {} clusters from initial clustering\".format(len(members)))\n",
    "    print(\"Initial clustering took:\", time.time() - s,\"seconds\")\n",
    "\n",
    "    return members, np.array(dists).squeeze()\n",
    "\n",
    "\n",
    "def count_numbers_of_sharedcontigs(dists, d1, min_shared_contigs):\n",
    "    s = time.time()\n",
    "    K, C = np.shape(dists)\n",
    "    shared = np.zeros([K,K])\n",
    "    # shared1 = np.zeros([K,K])\n",
    "    neigh = np.array(dists < d1).astype(int)\n",
    "    neigh_sum = neigh.sum(axis=0)\n",
    "    for c in np.arange(C):\n",
    "        if neigh_sum[c] >= 2:\n",
    "            non_zeroind = neigh[:,c].nonzero()\n",
    "            k_ind = np.array(np.meshgrid(non_zeroind,non_zeroind)).T.reshape(-1,2)\n",
    "            # k_ind = k_ind[k_ind[:,0] != k_ind[:,1]]\n",
    "            for i in k_ind:\n",
    "                shared[i[0],i[1]] += 1\n",
    "    shared = np.array(shared > min_shared_contigs).astype(int)\n",
    "\n",
    "    # for c in np.arange(C):\n",
    "\n",
    "    #     neigh = np.nonzero(np.array(dists)[:,c] < d1)[0]\n",
    "    #     if len(neigh) >= 2:\n",
    "    #         for k in neigh:\n",
    "    #             for l in neigh:\n",
    "    #                 shared[k,l] += 1\n",
    "\n",
    "    # shared = np.array(shared > min_shared_contigs).astype(int)\n",
    "    # print(shared1)\n",
    "\n",
    "    links = []\n",
    "    for k in np.arange(K):\n",
    "        links.append(list(np.nonzero(shared[k])))\n",
    "    print(\"count_numbers_of_sharedcontigs took \", time.time()-s, \"seconds\")\n",
    "    return links\n",
    "\n",
    "\n",
    "def recursive_set_allneighbors(k, component_curr, components, links):\n",
    "    components[k] = component_curr\n",
    "    for l in links[k][0]:\n",
    "\n",
    "        if components[l] < 0 :\n",
    "\n",
    "            recursive_set_allneighbors(l, component_curr, components, links)\n",
    "\n",
    "\n",
    "def find_connected_components(links):\n",
    "    s = time.time()\n",
    "    K = np.shape(links)[0]\n",
    "    components = np.zeros(K).astype(int) - 1\n",
    "    component_curr = 0\n",
    "\n",
    "    for k in np.arange(K):\n",
    "\n",
    "        if components[k] < 0:\n",
    "\n",
    "            recursive_set_allneighbors(k, component_curr, components, links)\n",
    "            component_curr += 1\n",
    "\n",
    "    if (component_curr != len(set(components))):\n",
    "        raise Exception(\"problem with component calculations\")\n",
    "        exit()\n",
    "    print(\"find_connected_components took \", time.time()-s, \"seconds\")\n",
    "    num_components = component_curr\n",
    "    numclust_incomponents = np.unique(components, return_counts=True)[1]\n",
    "    print(numclust_incomponents)\n",
    "    return components, num_components, numclust_incomponents\n",
    "\n",
    "\n",
    "def merge_members_by_connnected_components(components, num_components, members):\n",
    "    K = len(components)\n",
    "    clusters = [[] for i in range(num_components)]\n",
    "\n",
    "    for k in np.arange(K):\n",
    "        clusters[components[k]].append(members[k])\n",
    " \n",
    "    # for i in np.arange(num_components):\n",
    "        # clusters[i] = np.unique(np.concatenate(clusters[i]).ravel())\n",
    "       \n",
    "    return clusters\n",
    "\n",
    "\n",
    "def cluster_by_connecting_centroids(dat, d0, min_shared_contigs):\n",
    "    s = time.time()\n",
    "    contig_names = dat.columns\n",
    "    dat = dat.to_numpy()\n",
    "    N, C = np.shape(dat)\n",
    "\n",
    "    Rc    = dat.sum(axis=0)\n",
    "    Rn    = dat.sum(axis=1)\n",
    "\n",
    "    members, dists = cluster_by_centroids(dat, d0, N, C, Rc, Rn)\n",
    "    # d1 = d0 * np.sqrt(N)\n",
    "    d1 = d0 * 2\n",
    "    # d1 = d0 \n",
    "    links = count_numbers_of_sharedcontigs(dists, d1, min_shared_contigs)\n",
    "    components, num_components, numclust_incomponents = find_connected_components(links)\n",
    "    print(\"number of connected components\", num_components)\n",
    "    clusters = merge_members_by_connnected_components(components, num_components, members)\n",
    "\n",
    "    print(\"count_by_connecting_centroids took \", time.time()-s, \"seconds\")\n",
    "    return clusters, numclust_incomponents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    s = time.time()\n",
    "    print(\"clustering initiated\"+'\\n')\n",
    "    tmp_dir = \"/big/work/metadevol/benchmark_dataset1/\"\n",
    "    # tmp_dir = \"/big/work/metadevol/scripts/bamtools_api/build/\"\n",
    "    dat = pd.read_pickle(tmp_dir + 'X_pickle')\n",
    "    d0 = 1\n",
    "    min_shared_contigs = 5\n",
    "    clusters, numclust_incomponents = cluster_by_connecting_centroids(dat, d0, min_shared_contigs)\n",
    "    print(\"overall time taken for new clustering is: \", time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering initiated\n",
      "\n",
      "alpha optimized is: 8.445274503124452 and took 0.8735842704772949 seconds\n",
      "\n",
      "Obtained 37 clusters from initial clustering\n",
      "Initial clustering took: 2.3102469444274902 seconds\n",
      "count_numbers_of_sharedcontigs took  0.7183699607849121 seconds\n",
      "find_connected_components took  0.00038695335388183594 seconds\n",
      "[2 1 2 1 2 1 2 7 1 1 1 1 1 1 1 1 2 1 1 2 3 2]\n",
      "number of connected components 22\n",
      "count_by_connecting_centroids took  3.911520004272461 seconds\n",
      "overall time taken for new clustering is:  3.9179892539978027\n",
      "[array([    9,    22,    24, ..., 45750, 45752, 45759])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazhini/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from new_clustering_algorithm import cluster_by_connecting_centroids\n",
    "\n",
    "def initialize_Z(W_t, dat):\n",
    "    \n",
    "    W = np.transpose(W_t)\n",
    "    lmda = 0.1\n",
    "    inverse_term = np.linalg.inv(np.eye(W.shape[0]) + (lmda ** -1) * np.matmul(W,W_t))\n",
    "    woodbury = (lmda ** -1) * np.eye(W.shape[1]) - np.matmul((lmda ** -2) * W_t , np.matmul(inverse_term, W))\n",
    "    print(np.matmul(W_t,dat)[:,11977])\n",
    "    return np.matmul(woodbury, np.matmul(W_t,dat))\n",
    "\n",
    "\n",
    "# def maximize_function(w, z, x, qt):\n",
    "    \n",
    "#     mean = tf.matmul(w, z)\n",
    "#     mean = tf.math.maximum(0.1 * qt * x, 1e-20 + tf.nn.relu(mean))\n",
    "#     W_term = Lw * tf.reduce_sum(tf.abs(w))\n",
    "#     Z_term = Lz * tf.reduce_sum(tf.abs(z))\n",
    "#     block_penalty = Lzp * np.sqrt(sum(lc) / len(lc)) * sum(tf.norm(z / np.sqrt(lc), axis = 1))\n",
    "#     R = - W_term - Z_term - block_penalty\n",
    "#     negative_log_likelihood = - (tf.reduce_sum(- mean + tf.multiply(x, tf.math.log(mean))) + R)\n",
    "#     # negative_log_likelihood = - tf.reduce_sum(- mean + tf.multiply(x, tf.math.log(mean))) \n",
    "        \n",
    "#     return negative_log_likelihood\n",
    "\n",
    "\n",
    "def maximize_regularizedfunction(w, z, x, qt):\n",
    "    mean = tf.matmul(w, z)\n",
    "    mean = tf.math.maximum(0.1 * qt * x, 1e-20 + tf.nn.relu(mean))\n",
    "\n",
    "    \"\"\" L1 norm \"\"\"\n",
    "    W_term = Lw * tf.reduce_sum(tf.abs(w))\n",
    "    Rc = tf.reduce_sum(z, axis=0)\n",
    "    Z_term = Lz * tf.reduce_sum(tf.abs(z)/Rc)\n",
    "\n",
    "    \"\"\" block penalty \"\"\"\n",
    "    W_bterm = Lwp * tf.reduce_sum(tf.norm(w, axis=0))\n",
    "    Z_bterm = Lzp * tf.reduce_sum(tf.norm(z/Rc, axis=1))\n",
    "    R = - W_term - Z_term - W_bterm - Z_bterm\n",
    "\n",
    "    negative_log_likelihood = - (tf.reduce_sum(- mean + tf.multiply(x, tf.math.log(mean))) + R)\n",
    "        \n",
    "    return negative_log_likelihood\n",
    "\n",
    "\n",
    "def optimize_wz(W, Z, X, qt, opt):\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = maximize_regularizedfunction(W, Z, X, qt)\n",
    "        LL1.append(loss)\n",
    "    optimized = opt.minimize(loss, [W, Z], tape = tape)\n",
    "    \n",
    "    return optimized\n",
    "\n",
    "\n",
    "def calc_aic(w, z, x, qt):\n",
    "\n",
    "    mean = tf.matmul(w, z)\n",
    "    mean = tf.math.maximum(0.1 * qt * x, 1e-20 + tf.nn.relu(mean))\n",
    "    log_likelihood = tf.reduce_sum(- mean + tf.multiply(x, tf.math.log(mean)))\n",
    "    AIC_score = log_likelihood - tf.math.count_nonzero(tf.nn.relu(w), dtype=\"float64\") - tf.math.count_nonzero(tf.nn.relu(z), dtype=\"float64\")\n",
    "    \n",
    "    return AIC_score\n",
    "\n",
    "\n",
    "\n",
    "def nmf_with_adam(W, Z, X, n, revert_flag, convergence_criterion, Lw, Lz, Lzp, AIC_check_value):\n",
    "\n",
    "        LL1 = []\n",
    "        W_nzcount = []\n",
    "        Z_nzcount = []\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "        X = tf.convert_to_tensor(X)\n",
    "        X = tf.Variable(X, trainable = False)\n",
    "        W = tf.Variable(W, trainable = True)\n",
    "        Z = tf.Variable(Z, trainable = True)\n",
    "        W_prebest = tf.Variable(W, trainable = False)\n",
    "        Z_prebest = tf.Variable(Z, trainable = False)\n",
    "\n",
    "        for i in range(n):\n",
    "            qt = np.exp(-i/10)\n",
    "            optimize = optimize_wz(W, Z, X, qt, opt)\n",
    "            # if len(LL1) >= 150:\n",
    "            #     if abs((LL1[-1] - LL1[-100])/ LL1[-1]) < convergence_criterion and abs((LL1[-50] - LL1[-150])/ LL1[-50]) < convergence_criterion and revert_flag == 0:\n",
    "            #         AIC_score = calc_aic(W, Z, X, qt)\n",
    "            #         if AIC_score > AIC_check_value:\n",
    "            #             # opt = tf.keras.optimizers.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "            #             print(AIC_score, \"new AIC is higher than the old one\", AIC_check_value, i+1)\n",
    "            #             Lz = 2 * Lz\n",
    "            #             Lzp = 2 * Lzp\n",
    "            #             W_prebest.assign(W)\n",
    "            #             Z_prebest.assign(Z)\n",
    "            #             AIC_check_value = AIC_score\n",
    "            #             print(W_prebest[0][0], W[0][0], \" is set at\",  i+1)\n",
    "            #             print(Z_prebest[0][0], Z[0][0], \" is set at\",  i+1)\n",
    "            #             # revert_flag = 0\n",
    "\n",
    "            #         else:\n",
    "            #             W.assign(W_prebest)\n",
    "            #             print(\"else condition\", i)\n",
    "            # #             print(W_prebest[0][0], W[0][0], \"iteration \", i)\n",
    "            # #             print(Z_prebest[0][0], Z[0][0], \"iteration before correction\", i)\n",
    "            #             # Z.assign(tf.Variable(initialize_Z(tf.transpose(W), X)))\n",
    "            # #             # print(Z[0][0], \"iteration after correction\", i)\n",
    "            # #             # print(initialize_Z(tf.transpose(W), X))\n",
    "            #             Z.assign(Z_prebest)\n",
    "            #             Lz = Lz / 2\n",
    "            #             Lzp = Lzp / 2\n",
    "            #             revert_flag = 1\n",
    "\n",
    "            #     if revert_flag == 1:\n",
    "            #         if abs((LL1[-1] - LL1[-100])/ LL1[-1]) < convergence_criterion * 0.01 and abs((LL1[-50] - LL1[-150])/ LL1[-50]) < convergence_criterion * 0.01:\n",
    "            #             break\n",
    "\n",
    "            W.assign(tf.nn.relu(W))\n",
    "            W.assign(W / tf.reduce_sum(W, axis = 0))\n",
    "            Z.assign(tf.nn.relu(Z))\n",
    "\n",
    "            W_nzcount.append(tf.math.count_nonzero(W>0.01))\n",
    "            Z_nzcount.append(tf.math.count_nonzero(Z>0.01))\n",
    "\n",
    "\n",
    "            if (i+1 == 50 or i+1 == 100 or i+1 == 500 or i+1 == 1000 or i+1 == 9000 or i+1 == 10000):\n",
    "            #         print(np.count_nonzero(W>0.01, axis=0), \"W matrix at iteration\", i)\n",
    "        #         print(np.count_nonzero(Z>0.01, axis=1), \"Z matrix at iteration\", i)\n",
    "        #         np.save(\"/big/work/metadevol/scripts/bamtools_api/build/count_plot/W_matrix_trial\"+ str(i+1), W)\n",
    "        #         np.save(\"/big/work/metadevol/scripts/bamtools_api/build/count_plot/Z_matrix_trial\"+ str(i+1), Z)\n",
    "\n",
    "                Rc_c  = np.sum(Z, axis=0)\n",
    "                pb_c  = Z / Rc_c\n",
    "                cov_b = np.sum(Z, axis=1) / np.sum((np.array(lc) * Z) / Rc_c, axis=1)\n",
    "                # np.savetxt(tmp_dir + \"count_plot/bin_coverage_trial\"+ str(num_iterations),cov_b.reshape(len(cov_b),1), fmt=\"%f\")\n",
    "                pb_min = 0.8 * (cov_b.reshape(len(cov_b),1) * np.sum(np.square(pb_c), axis=0) \\\n",
    "                                / np.sum(cov_b.reshape(len(cov_b),1) * pb_c, axis=0))\n",
    "                pb_min[pb_min > 0.5] = 0.5\n",
    "                contig_assign0 = tf.argmax(pb_c/pb_min, axis=0).numpy()\n",
    "                print(len(set(contig_assign0)), \"new assignment at iteration\", i)\n",
    "                print(len(set(tf.argmax(pb_c, axis=0).numpy())), \"old assignment at iteration\", i)\n",
    "                Bz_bc =[]\n",
    "                for f in range(np.shape(Z)[1]):\n",
    "                    Bz_bc.append(Z[contig_assign0[f],f])\n",
    "                bins0 = np.c_[contig_assign0, np.array(Bz_bc).reshape(len(Bz_bc),1), np.array(contig_names)]\n",
    "        #         bins0 = bins0[np.lexsort((bins0[:,1], bins0[:,0]))]\n",
    "                print(\"Number of total bins \", len(set(bins0[:,0])), \" from new assignment automatic differentiation \\n\")\n",
    "                # np.savetxt(tmp_dir + \"count_plot/contig_bins_\" + str(i+1) + \"trial\", bins0, delimiter = \",\", fmt=['%d','%f','%d'])\n",
    "\n",
    "        return W, Z\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    s = time.time()\n",
    "    print(\"clustering initiated\"+'\\n')\n",
    "    tmp_dir = \"/big/work/metadevol/benchmark_dataset1/\"\n",
    "    # tmp_dir = \"/big/work/metadevol/scripts/bamtools_api/build/\"\n",
    "    dat = pd.read_pickle(tmp_dir + 'X_pickle')\n",
    "    d0 = 1\n",
    "    min_shared_contigs = 5\n",
    "    clusters, numclust_incomponents = cluster_by_connecting_centroids(dat, d0, min_shared_contigs)\n",
    "    print(\"overall time taken for new clustering is: \", time.time()-s)\n",
    "\n",
    "    # print(clusters)\n",
    "    dat = dat.to_numpy()\n",
    "    for k in numclust_incomponents:\n",
    "        connected_component_count = []\n",
    "        if k == 3 :\n",
    "            print(clusters[k])\n",
    "        #     for i in clusters[k]:\n",
    "        #         dat_s = dat[:, i]\n",
    "        #         connected_component_count.append(dat_s.sum(axis=1))\n",
    "\n",
    "        # if connected_component_count:\n",
    "        #     print(len(np.array(connected_component_count)), k)\n",
    "        #     print(np.array(connected_component_count))\n",
    "            # W_t = cluster_members_count/cluster_members_count.sum(axis=1,keepdims=1)\n",
    "            # Z_matrix = initialize_Z(W_t, dat)\n",
    "            # print(\"Initializing W and Z matrices took:\",time.time()-s1,'seconds\\n')\n",
    "\n",
    "            # print(\"Optimizing W and Z matrices\"+'\\n')\n",
    "\n",
    "            # s1 = time.time()\n",
    "            \n",
    "\n",
    "            # Lw = Lz = 1\n",
    "            # Lwp = 2 * Lw\n",
    "            # Lzp = 2 * Lw\n",
    "            \n",
    "            # AIC_check_value = 0.0\n",
    "\n",
    "            # convergence_criterion = 0.001\n",
    "            \n",
    "            # revert_flag = 0\n",
    "\n",
    "            # num_iterations = 1000\n",
    "\n",
    "            # \"\"\" W and Z optimization using SGD with automatic differentiation in Adam \"\"\"\n",
    "            # W1, Z1 = nmf_with_adam(np.transpose(W_t), Z_matrix, dat, num_iterations, revert_flag, convergence_criterion, Lw, Lz, Lzp, AIC_check_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, fi in zip(clusters, range(len(clusters))):\n",
    "    f = map(str,f)\n",
    "    for q in f:\n",
    "        with open(\"/big/work/metadevol/benchmark_dataset1/\" + \"connected_components_new\", 'a') as file:\n",
    "        # with open(\"/big/work/metadevol/scripts/bamtools_api/build/\" + \"connected_components_new\", 'a') as file:\n",
    "            file.write(str(q) + \" \" + str(fi) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  -  1\n",
      "0  -  4\n",
      "1  -  0\n",
      "1  -  4\n",
      "4  -  0\n",
      "4  -  1\n"
     ]
    }
   ],
   "source": [
    "for f in xx[xx[:,0] != xx[:,1]]:\n",
    "    print(f[0], \" - \", f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/big/work/metadevol/scripts/new_clustering_algorithm.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/big/work/metadevol/scripts/new_clustering_algorithm.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlist(itertools.permutations([0,1,4], 2))\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/big/work/metadevol/scripts/new_clustering_algorithm.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mtimeit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnp.array(np.meshgrid(np.array([0, 1, 4]),np.array([0, 1, 4]))).T.reshape(-1,2)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2285\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2283\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2284\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2285\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2286\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/execution.py:1166\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[1;32m   1164\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1166\u001b[0m all_runs \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mrepeat(repeat, number)\n\u001b[1;32m   1167\u001b[0m best \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(all_runs) \u001b[39m/\u001b[39m number\n\u001b[1;32m   1168\u001b[0m worst \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(all_runs) \u001b[39m/\u001b[39m number\n",
      "File \u001b[0;32m/usr/lib/python3.8/timeit.py:205\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m r \u001b[39m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat):\n\u001b[0;32m--> 205\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[1;32m    206\u001b[0m     r\u001b[39m.\u001b[39mappend(t)\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/execution.py:156\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    154\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    155\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    157\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%timeit list(itertools.permutations([0,1,4], 2))\n",
    "%timeit np.array(np.meshgrid(np.array([0, 1, 4]),np.array([0, 1, 4]))).T.reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/big/work/metadevol/scripts'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname(os.path.abspath(\"distance_calculations.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
